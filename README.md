# ğŸ¤– RAG Sandbox

> **R**etrieval **A**ugmented **G**eneration playground - Chat with AI about any webpage!

<div align="center">

![.NET](https://img.shields.io/badge/.NET-8.0-512BD4?style=for-the-badge&logo=dotnet)
![C#](https://img.shields.io/badge/C%23-239120?style=for-the-badge&logo=c-sharp)
![Ollama](https://img.shields.io/badge/Ollama-000000?style=for-the-badge&logo=ollama)

**A powerful API that lets you chat with AI about web content using local LLMs**

[Features](#-features) â€¢ [Quick Start](#-quick-start) â€¢ [API Reference](#-api-reference) â€¢ [Examples](#-examples)

</div>

---

## âœ¨ Features

| Feature | Description |
|---------|-------------|
| ğŸŒ **Web Scraping** | Automatically fetch and parse content from any URL |
| ğŸ§  **Multiple LLMs** | Support for various Ollama models (llama3, qwen, deepseek, etc.) |
| âš¡ **Streaming Responses** | Get real-time token-by-token responses |
| ğŸ’¬ **Session Management** | Maintain conversation context across requests |
| ğŸ¯ **Smart URL Parsing** | Automatically detect and extract URLs from messages |
| ğŸ”„ **RAG Pipeline** | Retrieve webpage content and augment LLM responses |

---

## ğŸš€ Quick Start

### Prerequisites

- [.NET 8.0 SDK](https://dotnet.microsoft.com/download/dotnet/8.0)
- [Ollama](https://ollama.ai/) installed and running
- At least one Ollama model pulled (e.g., `ollama pull llama3.2`)

### Installation

```bash
# Clone the repository
git clone <your-repo-url>
cd rag-sandbox

# Restore dependencies
dotnet restore

# Run the service
dotnet run
```

The service will start on **http://localhost:5247** ğŸ‰

---

## ğŸ“¡ API Reference

### ğŸ” Get Available Models

```http
GET /api/chat/models
```

**Response:**
```json
[
  {
    "name": "llama3.2:latest",
    "id": "a80c4f17...",
    "size": "1.9 GB",
    "modified": "12/04/2025 23:56"
  }
]
```

---

### ğŸ’¬ Chat (Standard)

```http
POST /api/chat
Content-Type: application/json
```

**Request Body:**
```json
{
  "message": "Based on this article https://example.com, tell me about...",
  "model": "qwen2.5:32b",
  "sessionId": "optional-session-id",
  "webUrl": "optional-direct-url"
}
```

**Response:**
```json
{
  "message": "AI response here...",
  "sessionId": "generated-session-id",
  "webUrl": "https://example.com",
  "model": "qwen2.5:32b"
}
```

---

### âš¡ Chat (Streaming)

```http
POST /api/chat/stream
Content-Type: application/json
```

**Request Body:**
```json
{
  "message": "Tell me a story about coding",
  "model": "llama3.2:latest"
}
```

**Response (Server-Sent Events):**
```
data: Once
data:  upon
data:  a
data:  time
data: ...
data: [DONE]
```

---

### ğŸ—‚ï¸ Get Session

```http
GET /api/chat/session/{sessionId}
```

**Response:**
```json
{
  "id": "session-id",
  "messages": [...],
  "webContentUrl": "https://example.com"
}
```

---

## ğŸ’¡ Examples

### Example 1: Simple Question

```bash
curl -X POST http://localhost:5247/api/chat \
  -H "Content-Type: application/json" \
  -d '{
    "message": "What is artificial intelligence?"
  }'
```

---

### Example 2: Chat About a Webpage

```bash
curl -X POST http://localhost:5247/api/chat \
  -H "Content-Type: application/json" \
  -d '{
    "message": "Summarize this article: https://en.wikipedia.org/wiki/Machine_learning"
  }'
```

---

### Example 3: Streaming Response

```bash
curl -N -X POST http://localhost:5247/api/chat/stream \
  -H "Content-Type: application/json" \
  -d '{
    "message": "Write a haiku about programming",
    "model": "qwen2.5:32b"
  }'
```

---

### Example 4: Continue Conversation

```bash
curl -X POST http://localhost:5247/api/chat \
  -H "Content-Type: application/json" \
  -d '{
    "message": "Tell me more about that",
    "sessionId": "previous-session-id"
  }'
```

---

## âš™ï¸ Configuration

### appsettings.json

```json
{
  "Logging": {
    "LogLevel": {
      "Default": "Information"
    }
  },
  "Ollama": {
    "BaseUrl": "http://localhost:11434",
    "DefaultModel": "llama3.2:latest"
  }
}
```

### Available Models

The service supports any model available in your local Ollama installation:

| Model | Size | Speed | Quality |
|-------|------|-------|---------|
| ğŸ¦™ llama3.2:latest | 1.9 GB | âš¡âš¡âš¡ | â­â­â­ |
| ğŸ¦™ llama3.1:latest | 4.6 GB | âš¡âš¡ | â­â­â­â­ |
| ğŸ§  deepseek-r1:14b | 8.4 GB | âš¡ | â­â­â­â­ |
| ğŸ¯ qwen2.5:32b | 18.5 GB | âš¡ | â­â­â­â­â­ |

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   HTTP Client   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ChatController â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ChatService   â”‚â”€â”€â”€â”€â”€â–¶â”‚ MessageParser    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â–¼                  â–¼                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ WebScrapingServiceâ”‚ â”‚ OllamaClient â”‚ â”‚SessionManagerâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¨ Features in Detail

### ğŸŒ Automatic URL Detection

The service automatically detects URLs in your messages:

```json
{
  "message": "Tell me about https://example.com, what does it say?"
}
```

The URL is extracted, scraped, and provided as context to the LLM.

---

### ğŸ’¾ Session Persistence

Conversations are maintained across requests using session IDs:

1. First request creates a new session
2. Response includes `sessionId`
3. Include `sessionId` in subsequent requests to continue the conversation

---

### âš¡ Streaming for Better UX

Streaming responses provide a better user experience, especially with large models:

- See responses as they're generated
- No timeout waiting for complete responses
- Perfect for chatbot interfaces

---

## ğŸ”§ Development

### Project Structure

```
rag-sandbox/
â”œâ”€â”€ Api/
â”‚   â””â”€â”€ Controllers/          # API endpoints
â”œâ”€â”€ Application/
â”‚   â”œâ”€â”€ Chat/                 # Chat service logic
â”‚   â””â”€â”€ WebContent/           # Web scraping
â”œâ”€â”€ Domain/
â”‚   â”œâ”€â”€ Chat/                 # Domain models
â”‚   â””â”€â”€ WebContent/
â”œâ”€â”€ Infrastructure/
â”‚   â”œâ”€â”€ LLM/                  # Ollama integration
â”‚   â””â”€â”€ WebScraping/          # HTML parsing
â””â”€â”€ Program.cs                # App entry point
```

### Run Tests

```bash
# Run all tests
dotnet test

# Run with coverage
dotnet test --collect:"XPlat Code Coverage"
```

---

## ğŸ› Troubleshooting

### Service won't start

**Problem:** Port 5247 already in use

**Solution:**
```bash
# Kill existing process
pkill -f "dotnet run"

# Or change port in launchSettings.json
```

---

### Ollama connection refused

**Problem:** Cannot connect to Ollama

**Solution:**
```bash
# Make sure Ollama is running
ollama serve

# Or check your Ollama BaseUrl in appsettings.json
```

---

### Slow responses with large models

**Problem:** qwen2.5:32b takes too long

**Solution:**
- Use streaming endpoint for better UX
- Or switch to a smaller model like llama3.2
- Timeout is set to 10 minutes (configurable in Program.cs)

---

## ğŸ¤ Contributing

Contributions are welcome! Feel free to:

- ğŸ› Report bugs
- ğŸ’¡ Suggest features
- ğŸ”§ Submit pull requests

---

## ğŸ“ License

This project is open source and available under the MIT License.

---

## ğŸ¯ Roadmap

- [ ] Add support for multiple URLs in a single message
- [ ] Implement document upload (PDF, DOCX)
- [ ] Add vector database for long-term memory
- [ ] Web UI for easier testing
- [ ] Docker support
- [ ] Add authentication/API keys

---

## ğŸ™ Acknowledgments

- **Ollama** - For making local LLMs accessible
- **HtmlAgilityPack** - For robust HTML parsing
- **.NET Team** - For an awesome framework

---

<div align="center">

**Made with â¤ï¸ and â˜•**

â­ Star this repo if you find it useful!

</div>
